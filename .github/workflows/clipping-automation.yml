name: Clipping FACIAP Automation

on:
  schedule:
    # Segunda a Sexta √†s 12h (15h UTC) e 20h (23h UTC) - Hor√°rio de Bras√≠lia
    - cron: '0 15,23 * * 1-5'  # 12h e 20h no hor√°rio de Bras√≠lia, apenas dias √∫teis
  workflow_dispatch:
    inputs:
      max_pages:
        description: 'M√°ximo de p√°ginas por fonte'
        required: false
        default: '3'
        type: string
      max_extraction:
        description: 'M√°ximo de extra√ß√µes por execu√ß√£o'
        required: false
        default: '50'
        type: string
      max_scoring:
        description: 'M√°ximo de scoring por execu√ß√£o'
        required: false
        default: '100'
        type: string
      force_execution:
        description: 'For√ßar execu√ß√£o mesmo fora do hor√°rio'
        required: false
        default: false
        type: boolean

env:
  TZ: America/Sao_Paulo

permissions:
  contents: read

concurrency:
  group: clipping-automation-${{ github.ref }}
  cancel-in-progress: true

jobs:
  clipping-automation:
    runs-on: ubuntu-latest
    timeout-minutes: 45
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Create directories
      run: |
        mkdir -p data logs
        
    - name: Restore database cache
      uses: actions/cache/restore@v4
      with:
        path: data/clipping_faciap.db
        key: clipping-db-${{ github.sha }}
        restore-keys: |
          clipping-db-
          
    - name: Execute clipping pipeline
      id: execute
      env:
        MAX_PAGES_PER_SOURCE: ${{ github.event.inputs.max_pages || '3' }}
        MAX_EXTRACTION_PER_RUN: ${{ github.event.inputs.max_extraction || '50' }}
        MAX_SCORING_PER_RUN: ${{ github.event.inputs.max_scoring || '100' }}
        FORCE_EXECUTION: ${{ github.event.inputs.force_execution || 'false' }}
        LOG_LEVEL: INFO
      run: |
        python - <<'PY'
        import os
        from datetime import datetime
        import pytz

        # Importa o pipeline do pacote scr
        from scr.pipeline import ClippingPipeline

        # Configura√ß√£o de timezone
        tz_brasil = pytz.timezone('America/Sao_Paulo')
        agora = datetime.now(tz_brasil)

        # Verifica se deve executar
        force_execution = str(os.getenv('FORCE_EXECUTION', 'false')).lower() == 'true'
        is_weekday = agora.weekday() < 5  # 0=Monday, 6=Sunday

        if not force_execution and not is_weekday:
            print(f'‚è≠Ô∏è Pulando execu√ß√£o - {agora.strftime("%%A")} n√£o √© dia √∫til')
            raise SystemExit(0)

        print('üöÄ INICIANDO PIPELINE AUTOMATIZADO')
        print('=' * 50)
        print(f'üìÖ Data/Hora: {agora.strftime("%%A, %%d/%%m/%%Y √†s %%H:%%M")}')
        print(f'üåç Timezone: {agora.tzname()}')
        print('üìä Par√¢metros:')
        print(f'   ‚Ä¢ Max p√°ginas por fonte: {os.getenv("MAX_PAGES_PER_SOURCE", "3")}')
        print(f'   ‚Ä¢ Max extra√ß√µes: {os.getenv("MAX_EXTRACTION_PER_RUN", "50")}')
        print(f'   ‚Ä¢ Max scoring: {os.getenv("MAX_SCORING_PER_RUN", "100")}')
        print()

        try:
            pipeline = ClippingPipeline()
            resultado = pipeline.executar_completo(
                max_pages_por_fonte=int(os.getenv('MAX_PAGES_PER_SOURCE', '3')),
                limite_extracao=int(os.getenv('MAX_EXTRACTION_PER_RUN', '50')),
                limite_scoring=int(os.getenv('MAX_SCORING_PER_RUN', '100'))
            )

            if resultado.get('sucesso'):
                print()
                print('‚úÖ PIPELINE CONCLU√çDO COM SUCESSO')
                print(f'‚è±Ô∏è Tempo total: {resultado.get("tempo_execucao", 0):.1f}s')
                print('üìä Resultados:')
                coleta = resultado.get('coleta', {})
                extracao = resultado.get('extracao', {})
                scoring = resultado.get('scoring', {})
                print(f"   üì∞ Not√≠cias coletadas: {coleta.get('total_coletadas', 0)}")
                print(f"   üÜï Not√≠cias novas: {coleta.get('total_novas', 0)}")
                print(f"   üìÑ Extra√ß√µes processadas: {extracao.get('processadas', 0)}")
                print(f"   üéØ Scoring processado: {scoring.get('processadas', 0)}")

                # Define sa√≠das para pr√≥ximos steps via GITHUB_OUTPUT
                if 'GITHUB_OUTPUT' in os.environ:
                    with open(os.environ['GITHUB_OUTPUT'], 'a', encoding='utf-8') as f:
                        print('success=true', file=f)
                        print(f"new_news={coleta.get('total_novas', 0)}", file=f)
                        print(f"execution_time={resultado.get('tempo_execucao', 0):.1f}", file=f)
            else:
                print('‚ùå PIPELINE FALHOU')
                print(f"‚ö†Ô∏è Erro: {resultado.get('erro', 'Erro desconhecido')}")
                raise SystemExit(1)
                
        except Exception as e:
            print(f'üí• ERRO CR√çTICO: {e}')
            import traceback
            traceback.print_exc()
            raise SystemExit(1)
        PY
          
    - name: Save database cache
      if: ${{ always() && hashFiles('data/clipping_faciap.db') != '' }}
      uses: actions/cache/save@v4
      with:
        path: data/clipping_faciap.db
        key: clipping-db-${{ github.sha }}
        
    - name: Upload database artifact
      if: ${{ always() && hashFiles('data/clipping_faciap.db') != '' }}
      uses: actions/upload-artifact@v4
      with:
        name: clipping-database-${{ github.run_number }}
        path: data/clipping_faciap.db
        retention-days: 30
        
    - name: Upload logs on failure
      if: failure()
      uses: actions/upload-artifact@v4
      with:
        name: automation-logs-${{ github.run_number }}
        path: logs/
        if-no-files-found: ignore
        retention-days: 7
        
    - name: Create execution summary
      if: always()
      run: |
        echo "## üìä Resumo da Execu√ß√£o" >> $GITHUB_STEP_SUMMARY
        echo "- **Data/Hora:** $(date '+%d/%m/%Y %H:%M %Z')" >> $GITHUB_STEP_SUMMARY
        echo "- **Status:** ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Dura√ß√£o:** ${{ steps.execute.outputs.execution_time || 'N/A' }}s" >> $GITHUB_STEP_SUMMARY
        echo "- **Not√≠cias novas:** ${{ steps.execute.outputs.new_news || 'N/A' }}" >> $GITHUB_STEP_SUMMARY
        
        if [ -f "data/clipping_faciap.db" ]; then
          db_size=$(du -h data/clipping_faciap.db | cut -f1)
          echo "- **Tamanho do banco:** $db_size" >> $GITHUB_STEP_SUMMARY
        else
          echo "- **Banco de dados:** N√£o encontrado" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### üîó Links √öteis" >> $GITHUB_STEP_SUMMARY
        echo "- [Dashboard Streamlit](https://clipping-legislativo-faciap.streamlit.app)" >> $GITHUB_STEP_SUMMARY
        echo "- [Reposit√≥rio](https://github.com/${{ github.repository }})" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ job.status }}" = "failure" ]; then
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ‚ö†Ô∏è Informa√ß√µes de Debug" >> $GITHUB_STEP_SUMMARY
          echo "- **Workflow:** ${{ github.workflow }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Run ID:** ${{ github.run_id }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Commit:** ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
        fi
